import argparse
import json
import os
import re
import concurrent.futures
import difflib
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, List, Tuple

from patchpilot.reproduce.task import make_swe_tasks
from patchpilot.util.utils import setup_logger, ensure_directory_exists
from patchpilot.util.model import make_model
from patchpilot.reproduce.task import parse_task_list_file

GOOD_TOKEN = "+"
BAD_TOKEN = "-"
PLACE_HOLDER = "<extra_0>"


def check_existing_verify_ids(verify_path):
    instance_ids = set()

    for root, _, files in os.walk(verify_path):
        for file in files:
            if file.endswith('verify_outputs.json'):
                file_path = os.path.join(root, file)
                with open(file_path, 'r', encoding='utf-8') as f:
                    try:
                        data = json.load(f)
                    except Exception as e:
                        print(f"Error loading {file_path}: {e}")
                        return instance_ids
                if 'instance_id' in data:
                    instance_ids.add(data['instance_id'])
    return instance_ids


class LLMVF:

    check_poc_prompt = """
    You are a code reviewer for a GitHub project. Your task is to evaluate whether a patch successfully resolves an issue described in a GitHub issue.

    You will be given the issue description, the PoC (Proof of Concept) code that demonstrates the issue, the PoC execution output before and after the patch.

    Your goal is to determine if the patch resolves the issue. Please respond with "Yes" or "No" and provide a brief explanation of your reasoning.
    You should not assume that there is other output that is not shown in the Poc Output. The Poc Output is the only output you should consider. You should not assume that a plot is generated by the Poc.

    - "Yes" means the patch successfully fixed the issue.
    - "No" means the patch did not successfully fix the issue, either because the issue still exists or because the patch introduced new issues.
    
    ### Raw Issue Description ###
    {issue_description}

    ### Poc code ###
    Here is the PoC code that demonstrates the issue:
    {poc_code}

    ### Poc Output before the patch ###
    {old_execution_output}
    
    ### Poc Output after the patch ###
    {new_execution_output}

    **Response Format:**


    Example 1:
    <judgement> Yes </judgement>
    <explanation> The patch successfully fixed the issue since ... </explanation>
    Example 2:
    <judgement> No </judgement>
    <explanation> The patch did not successfully fix the issue since ... </explanation>

    """

    def __init__(
            self,
            instance_id,
            problem_statement,
            model_name,
            backend,
            logger,
    ):
        self.instance_id = instance_id
        self.problem_statement = problem_statement
        self.max_tokens = 8192
        self.model_name = model_name
        self.backend = backend
        self.logger = logger


def extract_failed_tests(output_string):
    failed_tests = []
    for line in output_string.splitlines():
        if line.strip().endswith(' E') or '... ERROR' in line or "... FAIL" in line:
            failed_tests.append(line.strip())
        elif 'FAILED' in line and "failures=" not in line:
            failed_tests.append(line.strip())
        elif line.strip().startswith("test_") and (line.strip().endswith(' F') or line.strip().endswith(' f')):
            failed_tests.append(line.strip())
    return failed_tests


def filter_functionality_test_output(old_output, new_output):
    old_failed_tests = extract_failed_tests(old_output)
    new_failed_tests = extract_failed_tests(new_output)

    return old_failed_tests, new_failed_tests


def filter_poc_test_if_succeed(old_execution_output, new_execution_output):
    if 'error' in old_execution_output["stderr"].lower() or 'exception' in old_execution_output["stderr"].lower():
        # if the old execution output has error or exception, and the new execution output has no error or exception,
        # then the test is passed
        if 'error' not in new_execution_output["stderr"].lower() and 'exception' not in new_execution_output["stderr"].lower():
            return True
    if old_execution_output["stdout"] != "":
        # if there is no error or exception in the old execution output, if the old execution output has stdout,
        # and the new execution output has different stdout (without error), then the test is passed
        if 'error' not in new_execution_output["stderr"].lower() and 'exception' not in new_execution_output["stderr"].lower() and new_execution_output["stdout"] != old_execution_output["stdout"]:
            return True
    return False


def execute_poc_test(task, args, logger, reproduce_info):
    llm_judgement = False
    execution_output = task.execute_poc(reproduce_info)
    if len(execution_output["stdout"]) > 500:
        execution_output["stdout"] = execution_output["stdout"][-500:]
    if len(execution_output["stderr"]) > 500:
        execution_output["stderr"] = execution_output["stderr"][-500:]

    rp = LLMVF(
        task.task_id,
        reproduce_info["result"]["oracle"]["issue_description"],
        args.model,
        args.backend,
        logger,
    )
    print(reproduce_info["result"]["oracle"]["execution_output"])
    rule_based_judgement = filter_poc_test_if_succeed(reproduce_info["result"]["oracle"]["execution_output"],
                                                    execution_output)
    message = rp.check_poc_prompt.format(
        issue_description=task.problem_statement,
        poc_code=reproduce_info["result"]["poc"]["poc_code"],
        old_execution_output={
            "stdout": reproduce_info["result"]["oracle"]["execution_output"]["stdout"][-500:],
            "stderr": reproduce_info["result"]["oracle"]["execution_output"]["stderr"][-500:],
        },
        new_execution_output=execution_output,
    ).strip()
    rp.logger.info(f"prompting with message:\n{message}")
    print(f"prompting with message:\n{message}")
    rp.logger.info("=" * 80)
    print("=" * 80)

    model = make_model(
        model=rp.model_name,
        backend=rp.backend,
        logger=rp.logger,
        max_tokens=rp.max_tokens,
        temperature=0,
        batch_size=1,
    )
    if args.reasoning_mode:
        traj = model.codegen(message, num_samples=1, reasoning_mode=args.reasoning_mode, port=args.port, ip=args.ip)[0]
    else:
        traj = model.codegen(message, num_samples=1, port=args.port, ip=args.ip)[0]
    rp.logger.info(f"Got response:\n{traj}")
    traj["prompt"] = message
    raw_output = traj["response"]

    if args.reasoning_mode:
        verify_folder = args.verify_folder
        reasoning_output = os.path.join(verify_folder, f"reasoning_data.jsonl")
        reasoning_data = {
            "instance_id": task.task_id,
            "prompt": message,
            "reasoning_content": traj["reasoning_content"],
            "response": traj["response"],
        }

        with open(reasoning_output, "a") as f:
            f.write(json.dumps(reasoning_data) + "\n")

    print(raw_output)
    judgement = ""
    reason = ""
    try:
        judgement = raw_output.split("<judgement>")[1].split("</judgement>")[0].strip()
        reason = raw_output.split("<explanation>")[1].split("</explanation>")[0].strip()
    except Exception as e:
        print(
            "Error in parsing the output of the LLM, no judgement and reason found. The raw output is: {raw_output}")
        logger.error(
            f"Error in parsing the output of the LLM, no judgement and reason found. The raw output is: {raw_output}")
    if "yes" in judgement.lower():
        llm_judgement = True
    print(f"judgement: {judgement}, reason: {reason}")
    return execution_output, rule_based_judgement, llm_judgement, reason

def execute_verify_instance(task, args, existing_instance_ids, verify_folder):

    verify_info = {
        "instance_id": task.task_id,
        "result": {
            "poc_test_succeed_llm": [],
            "llm_judgement_reason": [],
            "poc_test_succeed_rule": [],
            "poc_is_executed": [],
            "poc_code": [],
            "poc_execution_output": [],
            "functionality_test_command": "",
            "functionality_test_output_ex": {
                "stdout": "",
                "stderr": "",
            },
            "functionality_test_output": {
                "stdout": "",
                "stderr": "",
            },
            "functionality_test_fail_num": {
                "old_failed_tests_num": 0,
                "new_failed_tests_num": 100,
            },
        }
    }

    log_file = os.path.join(
        verify_folder, f"{task.task_id}.log"
    )
    logger = setup_logger(log_file)
    logger.info(f"Processing bug {task.task_id}")

    if task.task_id in existing_instance_ids:
        print(f"Skip verifying existing instance_id: {task.task_id}")
        logger.info(f"Skip verifying existing instance_id: {task.task_id}")
        return
    
    verify_issue_id_folder = os.path.join(verify_folder, task.task_id)
    ensure_directory_exists(verify_issue_id_folder)
    verify_output_file = os.path.join(verify_issue_id_folder, "verify_outputs.json")
    
    task.setup_project()
    func_test_output_ex = task.execute_functionality_test()
    verify_info["result"]["functionality_test_output_ex"] = func_test_output_ex
    task.apply_patch()
    func_test_output = task.execute_functionality_test()
    verify_info["result"]["functionality_test_output"] = func_test_output
    verify_info["result"]["functionality_test_command"] = task.test_cmd
    verify_info["result"]["poc_is_executed"] = []
    verify_info["result"]["poc_execution_output"] = []
    verify_info["result"]["poc_test_succeed_rule"] = []
    verify_info["result"]["poc_test_succeed_llm"] = []
    verify_info["result"]["llm_judgement_reason"] = []
    verify_info["result"]["poc_code"] = []
    
    reproduce_issue_id_folder = os.path.join(args.reproduce_folder, task.task_id)
    ensure_directory_exists(reproduce_issue_id_folder)
    
    poc_index = 0
    while os.path.exists(os.path.join(reproduce_issue_id_folder, f"issue_parsing_report_{poc_index}.json")): 
        reproduce_output_file = os.path.join(reproduce_issue_id_folder, f"issue_parsing_report_{poc_index}.json")
        with open(reproduce_output_file, "r") as f:
            reproduce_info = json.load(f)
        verify_info["result"]["poc_code"].append(reproduce_info["result"]["poc"]["poc_code"])
        if reproduce_info["result"]["oracle"]["exec_match_wrong_behavior"]:  # if the generated PoC code matches the issue description
            poc_code_dict = reproduce_info["result"]["poc"]["poc_code"]
            _, poc_code = next(iter(poc_code_dict.items()))
            task.dump_poc(poc_code)
            execution_output, rule_based_judgement, llm_judgement, reason = execute_poc_test(task, args, logger, reproduce_info)
            verify_info["result"]["poc_is_executed"].append(True)
            verify_info["result"]["poc_execution_output"].append(execution_output)
            verify_info["result"]["poc_test_succeed_rule"].append(rule_based_judgement)
            verify_info["result"]["poc_test_succeed_llm"].append(llm_judgement)
            verify_info["result"]["llm_judgement_reason"].append(reason)
            verify_info["result"]["poc_code"].append(poc_code)
        else:
            poc_index += 1
            continue
        poc_index += 1
        if poc_index == args.poc_num:
            break

    old_test_output = verify_info["result"]["functionality_test_output_ex"]["stdout"] + verify_info["result"]["functionality_test_output_ex"]["stderr"]
    new_test_output = verify_info["result"]["functionality_test_output"]["stdout"] + verify_info["result"]["functionality_test_output"]["stderr"]
    old_failed_tests, new_failed_tests = filter_functionality_test_output(old_test_output, new_test_output)
    functionality_test_fail_num = {
        "old_failed_tests_num": len(old_failed_tests),
        "new_failed_tests_num": len(new_failed_tests)
    }
    diff = difflib.ndiff(old_failed_tests, new_failed_tests)
    diff_str = '\n'.join(line for line in diff if line.startswith(('+', '-')))
    verify_info["result"]["functionality_test_fail_num"] = functionality_test_fail_num
    verify_info["result"]["functionality_test_fail_diff_only_func"] = diff_str
    verify_info["result"]["old_failed_tests"] = '\n'.join(old_failed_tests)
    verify_info["result"]["new_failed_tests"] = '\n'.join(new_failed_tests)
    verify_info["patched_diff"] = task.patched_diff
    
    diff_whole = difflib.ndiff(old_test_output.splitlines(), new_test_output.splitlines())
    if len(old_test_output.splitlines()) > 1000 or len(new_test_output.splitlines()) > 1000:
        diff_whole = 'diff too long, skip'
    else:
        diff_whole = '\n'.join(line[1:] for line in diff_whole if line.startswith('+'))
    verify_info["result"]["functionality_test_fail_diff_whole"] = diff_whole

    with open(verify_output_file, "w") as ft:
        json.dump(verify_info, ft, indent=4)
    print("execute verifier for issue {} success! The result is in {}.".format(task.task_id, verify_output_file))


def load_prm(model_dir: str):
    """Load tokenizer / model and return objects + candidate/step token ids."""
    device = "cuda"
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForCausalLM.from_pretrained(model_dir).to(device).eval()

    # the ORM gives higher logit to GOOD_TOKEN when the patch is good
    candidate_tokens_good = tokenizer.encode(GOOD_TOKEN)
    candidate_tokens_bad = tokenizer.encode(BAD_TOKEN)
    candidate_tokens = candidate_tokens_good + candidate_tokens_bad  # index 0 is good

    # the score is taken at tokens that equal PLACE_HOLDER (or last token fallback)
    step_tag_id = tokenizer.encode(PLACE_HOLDER)[-1]

    return tokenizer, model, candidate_tokens, step_tag_id


# -----------------------------------------------------------------------------
# Helper: compute a single score for one patch diff using the ORM model
# -----------------------------------------------------------------------------
def score_patch(patch_text: str,
                 tokenizer,
                 model,
                 candidate_tokens: List[int],
                 step_tag_id: int) -> float:
    """Return probability the patch is GOOD according to ORM (higher = better)."""
    input_ids = torch.tensor([tokenizer.encode(patch_text)])

    with torch.no_grad():
        logits = model(input_ids).logits[:, :, candidate_tokens]  # (..., 2)
        probs_good = logits.softmax(dim=-1)[:, :, 0]             # select GOOD prob

    # if PLACE_HOLDER tokens exist, average their scores; else use last token score
    mask = input_ids == step_tag_id
    if mask.any():
        score = probs_good[mask].mean().item()
    else:
        score = probs_good[:, -1].item()

    return score


def prm_data_parser(generation_data: Dict[int, Dict[str, Dict[str, dict]]]):
    """Augment *generation_data* in‑place with PRM-formatted input strings.

    The function scans `generation_data[idx]["reasoning"]` and, for each
    `instance_id`, constructs a Qwen template string using `prompt` and
    `response` fields. The result is stored under `generation_data[idx]["prm"]`.
    """
    for idx, block in generation_data.items():
        reasoning = block.get("reasoning", {})
        prm_map = {}
        for iid, info in reasoning.items():
            try:
                prm_input = get_qwen_format_input(info["prompt"], info["response"])
            except KeyError as e:
                raise KeyError(f"Missing {e.args[0]} in reasoning for idx={idx}, iid={iid}") from e
            prm_map[iid] = {"prm_input": prm_input,
                            "instance_id": iid}
        block["prm"] = prm_map  # add alongside existing keys
    return generation_data


def get_qwen_format_input(prompt_message, answer):
    system_prompt = "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."

    prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
    prompt += f"<|im_start|>user\n{prompt_message}<|im_end|>\n"
    prompt += f"<|im_start|>assistant\n{answer} <extra_0> <im_end|>\n"

    return prompt


def parse_patch_reasoning(patch_folder: str) -> Dict[int, Dict[str, Dict[str, dict]]]:
    """Parse paired *output_i_processed.jsonl* & *reasoning_data_i.jsonl* files.

    Parameters
    ----------
    patch_folder : str
        Directory containing both kinds of files.

    Returns
    -------
    Dict[int, Dict[str, Dict[str, dict]]]
        ``{i: {"output": {instance_id: json_obj}, "reasoning": {instance_id: json_obj}}}``
        Only indices *i* that have **both** files are kept.
    """
    folder = Path(patch_folder)
    if not folder.is_dir():
        raise FileNotFoundError(f"patch_folder not found or not a dir: {patch_folder}")

    data: Dict[int, Dict[str, Dict[str, dict]]] = {}
    for out_path in folder.glob("output_*_processed.jsonl"):
        m = re.match(r"output_(\d+)_processed\.jsonl", out_path.name)
        if not m:
            continue
        idx = int(m.group(1))
        reason_path = folder / f"reasoning_data_{idx}.jsonl"
        if not reason_path.exists():
            continue

        # --- read output_i_processed.jsonl ---
        output_map: Dict[str, dict] = {}
        with open(out_path, "r", encoding="utf-8") as f_out:
            for line in f_out:
                if not line.strip():
                    continue
                obj = json.loads(line)
                iid = obj.get("instance_id")
                if iid is None:
                    raise ValueError(f"Missing instance_id in {out_path}: {obj}")
                output_map[iid] = obj

        # --- read reasoning_data_i.jsonl ---
        reasoning_map: Dict[str, dict] = {}
        with open(reason_path, "r", encoding="utf-8") as f_rsn:
            for line in f_rsn:
                if not line.strip():
                    continue
                obj = json.loads(line)
                iid = obj.get("instance_id")
                if iid is None:
                    raise ValueError(f"Missing instance_id in {reason_path}: {obj}")
                reasoning_map[iid] = obj

        data[idx] = {"output": output_map, "reasoning": reasoning_map}

    return data


def _score_single(prm_input: str, tokenizer, model, cand_tok: torch.Tensor, step_tag_id: int, device):
    ids = tokenizer.encode(prm_input, return_tensors="pt").to(device)
    try:
        with torch.inference_mode():
            probs = model(ids).logits[:, :, cand_tok].softmax(dim=-1)[:, :, 0]
        step_scores = probs[ids == step_tag_id]
        print(step_scores)
        if step_scores.numel() == 0:
            return 0.0  # <extra_0> missing
        return step_scores[-1].item()
    except IndexError:
        # catches rare indexing error from huggingface when len > max_len
        return 0.0


def score_prm_batch(prm_inputs: List[str], tokenizer, model, cand_tok: torch.Tensor, step_tag_id: int, device, workers: int = 4):
    if workers <= 1:
        return [_score_single(p, tokenizer, model, cand_tok, step_tag_id, device) for p in prm_inputs]
    results = [0.0] * len(prm_inputs)
    with ThreadPoolExecutor(max_workers=workers) as pool:
        fut_idx = {pool.submit(_score_single, p, tokenizer, model, cand_tok, step_tag_id, device): i for i, p in enumerate(prm_inputs)}
        for fut in as_completed(fut_idx):
            results[fut_idx[fut]] = fut.result()
    return results


def save_prm_result(verify_folder: str, idx:int, block: Dict):
    out_dir = Path(verify_folder)
    out_dir.mkdir(parents=True, exist_ok=True)

    prm_map = block.get("prm", {})
    out_path = out_dir / f"prm_result_{idx}.jsonl"
    with open(out_path, "w", encoding="utf-8") as f:
        for iid, info in prm_map.items():
            record = {"instance_id": iid, **info}  # info contains prm_input & prm_value
            f.write(json.dumps(record, ensure_ascii=False) + "\n")


def check_prm_result(verify_folder: str, idx: int) -> bool:
    """Return True if prm_result_{idx}.jsonl exists and non‑empty."""
    fp = Path(verify_folder) / f"prm_result_{idx}.jsonl"
    return fp.is_file() and fp.stat().st_size > 0


def prm_ranking(args, generation_data):
    prm_data_parser(generation_data)
    prm_dir = args.prm_dir
    tokenizer, model, candidate_tokens, step_tag_id = load_prm(prm_dir)
    device = model.device
    workers = args.num_threads

    for idx, block in generation_data.items():
        print("=====Start PRM Calculation for ", idx, "=====")
        if check_prm_result(args.verify_folder, idx):
            print("Already finished ", idx, ", skipped\n")
            continue
        iids, inputs = zip(*[(iid, info["prm_input"]) for iid, info in block["prm"].items()])
        scores = score_prm_batch(list(inputs), tokenizer, model, candidate_tokens, step_tag_id, device, workers)
        for iid, s in zip(iids, scores):
            block["prm"][iid]["prm_value"] = s
        save_prm_result(args.verify_folder, idx, block)
        print("Already finished ", idx)
    return generation_data


def dynamic_test(args, patch_file, verify_folder):
    patch_list = []
    with open(patch_file, "r") as f:
        json_list = list(f)
    for json_str in json_list:
        result = json.loads(json_str)
        patch_list.append(result)

    for patch in patch_list:
        patch_instance_id = patch.get("instance_id")
        patch_diff = patch.get("model_patch")
        if patch_diff:
            patch_diff += "\n"
        for task in args.tasks_list:
            if task.task_id == patch_instance_id:
                task.patched_diff = patch_diff
    existing_instance_ids = (
        check_existing_verify_ids(verify_folder)
    )

    if args.num_threads == 1:
        for task in args.tasks_list:
            execute_verify_instance(
                task, args, existing_instance_ids, verify_folder
            )
    else:
        with concurrent.futures.ThreadPoolExecutor(
                max_workers=args.num_threads
        ) as executor:
            futures = [
                executor.submit(
                    execute_verify_instance,
                    task,
                    args,
                    existing_instance_ids,
                    verify_folder
                )
                for task in args.tasks_list
            ]
            concurrent.futures.wait(futures)
            for fut in futures:
                exc = fut.exception()
                if exc is not None:
                    print(f"Task raised an exception: {exc}")
                else:
                    res = fut.result() 


def verify(args):
    if args.enable_prm:
        generation_data = parse_patch_reasoning(args.patch_folder)
        prm_result = prm_ranking(args, generation_data)
        # dynamic_test(args)
    else:
        patch_folder = Path(args.patch_folder)
        output_pairs = []
        for out_path in patch_folder.glob("output_*_processed.jsonl"):
            m = re.match(r"output_(\d+)_processed\.jsonl$", out_path.name)
            if not m:
                continue
            idx = int(m.group(1))
            output_pairs.append((idx, out_path))
        output_pairs.sort(key=lambda kv: kv[0])
        output_files = [fp for _, fp in output_pairs]

        base_idx = getattr(args, "num_generated_sample", 0)
        for i, out_fp in enumerate(output_files):
            # verify_folder/samples_<N>
            v_dir = Path(args.verify_folder) / f"samples_{base_idx + i}"
            v_dir.mkdir(parents=True, exist_ok=True)
            dynamic_test(args, str(out_fp), str(v_dir))


def args_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument("--reasoning_mode", action="store_true", default=False)
    parser.add_argument("--rerank", type=bool, default=False)
    parser.add_argument("--verify_folder", type=str, required=True)
    parser.add_argument("--reproduce_folder", type=str, required=True)
    parser.add_argument("--patch_folder", type=str, required=True)
    parser.add_argument("--prm_dir", type=str)
    parser.add_argument("--enable_prm", action="store_true", default=False)
    parser.add_argument("--poc_num", type=int, default=5)
    parser.add_argument(
        "--setup_map",
        type=str,
        required=True,
        help="Path to json file that contains the setup information of the projects.",
    )
    parser.add_argument(
        "--tasks_map",
        type=str,
        required=True,
        help="Path to json file that contains the tasks information.",
    )
    parser.add_argument(
        "--task_list_file",
        type=str,
        help="Path to the file that contains all tasks ids to be run.",
    )
    parser.add_argument("--target_id", type=str)
    parser.add_argument(
        "--num_threads",
        type=int,
        default=1,
        help="Number of threads to use for creating API requests",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="UCSB-SURFI/Co-PatcheR-Val-no-assert-14B",
    )
    parser.add_argument(
        "--backend", type=str, default="opensource", choices=["openai", "deepseek", "claude", "opensource"]
    )
    parser.add_argument("--prm_model", type=str)
    parser.add_argument("--ip", type=str, default="0.0.0.0")
    parser.add_argument("--port", type=int, default=2951)

    args = parser.parse_args()

    os.makedirs(args.verify_folder, exist_ok=True)

    # write the arguments
    with open(f"{args.verify_folder}/verify_args.json", "w") as f:
        json.dump(vars(args), f, indent=4)

    assert not (args.target_id is not None and args.task_list_file is not None), "Cannot specify both task and task-list."
    all_task_ids = []
    if args.task_list_file is not None:
        all_task_ids = parse_task_list_file(args.task_list_file)
    elif args.target_id is not None:
        all_task_ids = [args.target_id]
    assert len(all_task_ids) > 0, "No task ids to run."
    args.tasks_list = make_swe_tasks(all_task_ids, args.setup_map, args.tasks_map)

    return args


def main():
    args = args_parser()

    verify(args)


if __name__ == "__main__":
    main()
